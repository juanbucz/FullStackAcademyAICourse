{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022dafe7",
   "metadata": {},
   "source": [
    "# **===========================================================================================================================================================** #\n",
    "# **Project Name** #\n",
    "# **===========================================================================================================================================================** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cdbb2",
   "metadata": {},
   "source": [
    "## <u>**Notebook Setup**</u> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f38935c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "from scipy.stats import boxcox, yeojohnson, shapiro\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    KBinsDiscretizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    MinMaxScaler,\n",
    "    StandardScaler,\n",
    "    OrdinalEncoder,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    PolynomialFeatures\n",
    ")\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# How to tell python version\n",
    "#print (sys.version_info)\n",
    "#print (platform.python_version())\n",
    "\n",
    "# How to pip install from Terminal window:\n",
    "# python -m pip install seaborn\n",
    "\n",
    "# Put this at the top of your notebook\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)  # Set a large number\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(315)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bcde46",
   "metadata": {},
   "source": [
    "## <u>**1. Data Import and Initial Exploratory Data Analysis**</u> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a121a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************\n",
      "Description Stats\n",
      "***********************************\n",
      "\n",
      "        count unique         top  freq          mean           std     min      25%      50%      75%       max\n",
      "Date     7560     90  1-Oct-2020    84           NaN           NaN     NaN      NaN      NaN      NaN       NaN\n",
      "Time     7560      3     Morning  2520           NaN           NaN     NaN      NaN      NaN      NaN       NaN\n",
      "State    7560      7          WA  1080           NaN           NaN     NaN      NaN      NaN      NaN       NaN\n",
      "Group    7560      4        Kids  1890           NaN           NaN     NaN      NaN      NaN      NaN       NaN\n",
      "Unit   7560.0    NaN         NaN   NaN     18.005423     12.901403     2.0      8.0     14.0     26.0      65.0\n",
      "Sales  7560.0    NaN         NaN   NaN  45013.558201  32253.506944  5000.0  20000.0  35000.0  65000.0  162500.0\n",
      "\n",
      "***********************************\n",
      "Basic Info of imported data set\n",
      "***********************************\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7560 entries, 0 to 7559\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    7560 non-null   object\n",
      " 1   Time    7560 non-null   object\n",
      " 2   State   7560 non-null   object\n",
      " 3   Group   7560 non-null   object\n",
      " 4   Unit    7560 non-null   int64 \n",
      " 5   Sales   7560 non-null   int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 354.5+ KB\n",
      "\n",
      "\n",
      "df_ausapparalsales Shape:(7560, 6)\n",
      "\n",
      "Do we have any missing values:\n",
      "False\n",
      "\n",
      "Feature Columns with that have missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Do we have any zero length stripped values -- empties values:\n",
      "Date     False\n",
      "Time     False\n",
      "State    False\n",
      "Group    False\n",
      "Unit     False\n",
      "Sales    False\n",
      "dtype: bool\n",
      "\n",
      "Do we have any numeric columns with zero (0) values - which could be missing data?\n",
      "Unit     False\n",
      "Sales    False\n",
      "dtype: bool\n",
      "***********************************\n",
      "First 20 rows of Data\n",
      "***********************************\n",
      "\n",
      "          Date        Time State     Group  Unit  Sales\n",
      "0   1-Oct-2020     Morning    WA      Kids     8  20000\n",
      "1   1-Oct-2020     Morning    WA       Men     8  20000\n",
      "2   1-Oct-2020     Morning    WA     Women     4  10000\n",
      "3   1-Oct-2020     Morning    WA   Seniors    15  37500\n",
      "4   1-Oct-2020   Afternoon    WA      Kids     3   7500\n",
      "5   1-Oct-2020   Afternoon    WA       Men    10  25000\n",
      "6   1-Oct-2020   Afternoon    WA     Women     3   7500\n",
      "7   1-Oct-2020   Afternoon    WA   Seniors    11  27500\n",
      "8   1-Oct-2020     Evening    WA      Kids    15  37500\n",
      "9   1-Oct-2020     Evening    WA       Men    15  37500\n",
      "10  1-Oct-2020     Evening    WA     Women     3   7500\n",
      "11  1-Oct-2020     Evening    WA   Seniors    10  25000\n",
      "12  1-Oct-2020     Morning    NT      Kids    13  32500\n",
      "13  1-Oct-2020     Morning    NT       Men     5  12500\n",
      "14  1-Oct-2020     Morning    NT     Women     4  10000\n",
      "15  1-Oct-2020     Morning    NT   Seniors    10  25000\n",
      "16  1-Oct-2020   Afternoon    NT      Kids    13  32500\n",
      "17  1-Oct-2020   Afternoon    NT       Men     4  10000\n",
      "18  1-Oct-2020   Afternoon    NT     Women     6  15000\n",
      "19  1-Oct-2020   Afternoon    NT   Seniors     5  12500\n",
      "\n",
      "***********************************\n",
      "First 20 rows of Random Sample Data\n",
      "***********************************\n",
      "\n",
      "             Date        Time State     Group  Unit   Sales\n",
      "1556  19-Oct-2020     Evening   VIC      Kids    39   97500\n",
      "1430  18-Oct-2020     Morning    WA     Women     6   15000\n",
      "5379   5-Dec-2020     Morning    WA   Seniors    14   35000\n",
      "176    3-Oct-2020     Evening    WA      Kids     3    7500\n",
      "2279  28-Oct-2020     Evening    WA   Seniors     3    7500\n",
      "6738  21-Dec-2020   Afternoon    NT     Women    14   35000\n",
      "2532   1-Nov-2020     Morning    NT      Kids     4   10000\n",
      "1754  21-Oct-2020     Morning   TAS     Women     3    7500\n",
      "5657   8-Dec-2020   Afternoon    SA       Men    24   60000\n",
      "1453  18-Oct-2020     Morning    SA       Men    32   80000\n",
      "2027  25-Oct-2020     Evening    WA   Seniors    11   27500\n",
      "2739   3-Nov-2020     Morning   QLD   Seniors    10   25000\n",
      "3424  11-Nov-2020   Afternoon   NSW      Kids    19   47500\n",
      "5764   9-Dec-2020   Afternoon   QLD      Kids    11   27500\n",
      "6389  17-Dec-2020   Afternoon    WA       Men     5   12500\n",
      "6312  16-Dec-2020     Morning    NT      Kids    12   30000\n",
      "1424  17-Oct-2020     Evening   TAS      Kids    11   27500\n",
      "2193  27-Oct-2020     Evening    WA       Men     5   12500\n",
      "819   10-Oct-2020     Morning   NSW   Seniors    40  100000\n",
      "2042  25-Oct-2020     Morning    SA     Women    27   67500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data into DataFrame\n",
    "# Renamed CSV file is assumed to live in same folder as my notebook\n",
    "df_ausapparalsales = pd.read_csv(\"AusApparalSales4thQrt2020.csv\")\n",
    "\n",
    "# Make a copy of the dataframe to contain the modifications completed in this workbook\n",
    "df_ausapparalsales_new = df_ausapparalsales.copy(deep=True)\n",
    "\n",
    "# Let's look at what we just imported and get some more info\n",
    "# I like to delineate output to make it more readable\n",
    "\n",
    "# Print Stats\n",
    "print(\"***********************************\")\n",
    "print(\"Description Stats\")\n",
    "print(\"***********************************\")\n",
    "print()\n",
    "print(df_ausapparalsales.describe(include='all').T)\n",
    "print()\n",
    "\n",
    "# Print df Column Info\n",
    "print(\"***********************************\")\n",
    "print(\"Basic Info of imported data set\")\n",
    "print(\"***********************************\")\n",
    "print()\n",
    "df_ausapparalsales.info()\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "print(f'df_ausapparalsales Shape:{df_ausapparalsales.shape}')\n",
    "print()\n",
    "\n",
    "print('Do we have any missing values:')\n",
    "print(df_ausapparalsales.isnull().any().any())\n",
    "print()\n",
    "\n",
    "print('Feature Columns with that have missing values:')\n",
    "print(df_ausapparalsales.isnull().sum()[df_ausapparalsales.isnull().sum() > 0])\n",
    "print()\n",
    "\n",
    "# Check to see if any values (stripped of white space) contain a zero length string\n",
    "print('Do we have any zero length stripped values -- empties values:')\n",
    "print((df_ausapparalsales.astype(str).apply(lambda col: col.str.strip() == '')).any())\n",
    "print()\n",
    "\n",
    "# Check numeric columns for 0 (potentially missing data) values\n",
    "numeric_cols = df_ausapparalsales.select_dtypes(include=['int', 'int64', 'float']).columns\n",
    "print('Do we have any numeric columns with zero (0) values - which could be missing data?')\n",
    "print((df_ausapparalsales[numeric_cols] == 0).any())\n",
    "print()\n",
    "\n",
    "print(\"***********************************\")\n",
    "print(\"First 20 rows of Data\")\n",
    "print(\"***********************************\")\n",
    "print()\n",
    "print(df_ausapparalsales.head(20))\n",
    "print()\n",
    "\n",
    "print(\"***********************************\")\n",
    "print(\"First 20 rows of Random Sample Data\")\n",
    "print(\"***********************************\")\n",
    "print()\n",
    "print(df_ausapparalsales.sample(20))\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73828de5",
   "metadata": {},
   "source": [
    "#### <b>Results of Initial Investigation</b> ####\n",
    "    - We have 7,560 rows of data\n",
    "    - The dataset contains 5 columns (features)\n",
    "    - Initial testing reveals that we don't have any missing data/empty values\n",
    "    - The column types look correct (as what we'd expect) except for Date and Time\n",
    "      We should investigate translating these into Date and Time:\n",
    "        - Date (object) => Date\n",
    "        - Time (object) => Time\n",
    "    - For more of potential trend revealing views we probably want to investigate binning\n",
    "      these values into:\n",
    "        - Date => Season (Fall, Winter, Spring, Summer)\n",
    "        - Time => TimeOfDay (Morning, Afternoon, Evening, Night)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b464cf",
   "metadata": {},
   "source": [
    "#### Potential Data Issues from Imported Data ####\n",
    "    1. The following columns (features) have missing data:\n",
    "        visits      \n",
    "        nvisits     \n",
    "\n",
    "\n",
    "    2. Data Types \n",
    "        The following data types should be converted to integers - for performance purposes\n",
    "            visits     4186 non-null   float64\n",
    "            nvisits    4186 non-null   float64\n",
    "            ovisits    4186 non-null   float64\n",
    "        The additional data type conversions/fixups should probably be done:\n",
    "        These conversions should be done before/while we convert from float to int\n",
    "            age -> needs to be normalized to it's absolulte/actual value - suspicious 7.67 probably multiply by 10\n",
    "            income -> same as age\n",
    "        These columns should be made into categorical variables:\n",
    "            chronic -> category (object) -> ordinal value\n",
    "        These columns should be converted from yes/no to bool (True/False) and/or (1/0)\n",
    "            employed\n",
    "            insured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e07b9",
   "metadata": {},
   "source": [
    "### Plot the original \"Dirty Data\" to see distributions ###\n",
    "#### Feature Distributions for Numerical Columns-- Histograms ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47fed4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Feature Columns to plot\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_NSME1988_dirty' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# The columns we want to plot:\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mNumeric Feature Columns to plot\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m numeric_cols = \u001b[43mdf_NSME1988_dirty\u001b[49m.select_dtypes(include=[\u001b[33m'\u001b[39m\u001b[33mnumber\u001b[39m\u001b[33m'\u001b[39m]).columns.tolist()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(numeric_cols)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPlotHistogramsToViewDistributions\u001b[39m(df_dataset):\n",
      "\u001b[31mNameError\u001b[39m: name 'df_NSME1988_dirty' is not defined"
     ]
    }
   ],
   "source": [
    "# The columns we want to plot:\n",
    "print('Numeric Feature Columns to plot')\n",
    "numeric_cols = df_NSME1988_dirty.select_dtypes(include=['number']).columns.tolist()\n",
    "print(numeric_cols)\n",
    "\n",
    "def PlotHistogramsToViewDistributions(df_dataset):\n",
    "    n_cols = len(numeric_cols)\n",
    "    print(f'Number of Columns: {n_cols}')\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=(n_cols + 2) // 3, ncols=3, figsize=(15, 5 * ((n_cols + 2) // 3)))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plt.suptitle('Raw Feature Distributions - Non-Normailzed', fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])  # Leave space at top\n",
    "\n",
    "    #Iterate over each numeric column and plot a historgram\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        sns.histplot(data=df_dataset, x=col, ax=axes[i], kde=True)    \n",
    "        axes[i].set_title(col, fontweight='bold')\n",
    "\n",
    "        # Calculate statistics\n",
    "        mean_val = df_dataset[col].mean()\n",
    "        median_val = df_dataset[col].median()\n",
    "        mode_val = df_dataset[col].mode()[0] if not df_dataset[col].mode().empty else None\n",
    "        std_val = df_dataset[col].std()\n",
    "\n",
    "        # Plot Statistics as Vertical Lines\n",
    "        axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "        axes[i].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "        if mode_val is not None:\n",
    "            axes[i].axvline(mode_val, color='blue', linestyle='--', linewidth=2, label=f'Mode: {mode_val:.2f}')\n",
    "        \n",
    "        # Add std as shaded region around mean\n",
    "        axes[i].axvspan(mean_val - std_val, mean_val + std_val, alpha=0.2, color='gray', label=f'Std: {std_val:.2f}')\n",
    "        \n",
    "        # Add legend\n",
    "        axes[i].legend(fontsize=8, loc='upper right')\n",
    "\n",
    "\n",
    "    # Make plots more readable and add space\n",
    "    plt.tight_layout(h_pad=5)\n",
    "    plt.show()   \n",
    "\n",
    "\n",
    "PlotHistogramsToViewDistributions(df_NSME1988_dirty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b5d60",
   "metadata": {},
   "source": [
    "##### Histogram Analysis #####\n",
    "    1. The following columns' distribution is right skewed (indicating the presence of outliers).\n",
    "        This will have to be fixed via cropping as we seek a more normalized distribution.\n",
    "        visits      \n",
    "        nvisits     \n",
    "        ovisits     \n",
    "        novisits    \n",
    "        emergency   \n",
    "        hospital    \n",
    "        chronic     \n",
    "        age         \n",
    "        school      \n",
    "        income       \n",
    "        insurance\n",
    "    2. Age does not follow this pattern. It looks like a more normal or bimodal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319578c",
   "metadata": {},
   "source": [
    "#### Box Plots for Numerical Columns-- Identify Outliers ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a695762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PlotBoxPlotsToViewDistributions(df_dataset):\n",
    "    n_cols = len(numeric_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=(n_cols + 2) // 3, ncols=3, figsize=(15, 5 * ((n_cols + 2) // 3)))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plt.suptitle('Raw Feature Distributions - Box Plots', fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])  # Leave space at top\n",
    "\n",
    "    #Iterate over each numeric column and plot a historgram\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        sns.boxplot(data=df_dataset, y=col, ax=axes[i])    \n",
    "        axes[i].set_title(col, fontweight='bold')\n",
    "\n",
    "        # Optional: Add count of outliers\n",
    "        Q1 = df_dataset[col].quantile(0.25)\n",
    "        Q3 = df_dataset[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        outlier_count = len(df_dataset[(df_dataset[col] < lower) | \n",
    "                                            (df_dataset[col] > upper)])\n",
    "        axes[i].text(0.5, 0.95, f'Outliers: {outlier_count}', \n",
    "                    transform=axes[i].transAxes, ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "    # Make plots more readable and add space\n",
    "    plt.tight_layout(h_pad=5)\n",
    "    plt.show()   \n",
    "\n",
    "PlotBoxPlotsToViewDistributions(df_NSME1988_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b333fe",
   "metadata": {},
   "source": [
    "##### BoxPlot Analysis #####\n",
    "    1. The boxplots reinforce the histograms results, indicating outliers in the higher values (which relate to right skewedness).\n",
    "    The outliers indicate a high volume of outliers (lots of circles above the box).\n",
    "    The boxes for most of them look pretty small which indicates the median is close to zero (0).\n",
    "    It looks like healthcare resources aren't heavily used.\n",
    "        visits      \n",
    "        nvisits     \n",
    "        ovisits     \n",
    "        novisits    \n",
    "        emergency   \n",
    "        hospital    \n",
    "        chronic     \n",
    "        age         \n",
    "        school      \n",
    "        income       \n",
    "        insurance\n",
    "    2. School looks like it's got some outliers at the lower values, which would point to a left skew with some lower valued outliers.\n",
    "       I don't think we want to fix (crop) this because lower education could very well be a predicator and have some significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine value ranges for numeric columns\n",
    "#\n",
    "\n",
    "summary = pd.DataFrame()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    summary = df_NSME1988_dirty[numeric_cols].describe().loc[['min', '25%', '50%', '75%', 'max']]\n",
    "    \n",
    "print(summary)\n",
    "\n",
    "sns.heatmap(df_NSME1988_dirty.isnull(), cbar=False, yticklabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd081b",
   "metadata": {},
   "source": [
    "##### Range Analysis #####\n",
    "    1. We have some bad/incorrect data with the following columns registering a count 0f -1 (You can't have -1 visits)\n",
    "        visits      \n",
    "        nvisits     \n",
    "        ovisits     \n",
    "        novisits    \n",
    "    2. The following columns look suspect because 3/4 of their quantiles are showing 0 with values skewed to max\n",
    "        emergency\n",
    "        hospital\n",
    "    3. I can do a heatmap to show the missing data in the dataframe (numeric columns).\n",
    "    The heatmap is a plot of missing data in the numeric columns. Visually it looks like we have a fair amount of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bac1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# grubbs_test - Found this on internet when searching for methods\n",
    "#\n",
    "#\n",
    "def grubbs_test(data):\n",
    "    \"\"\"\n",
    "    Performs Grubbs' test for outliers on a dataset\n",
    "    Tests if the most extreme value is an outlier\n",
    "    \n",
    "    Returns: G statistic and p-value\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data, ddof=1)\n",
    "    n = len(data)\n",
    "    \n",
    "    # Calculate G statistic (max deviation from mean in std units)\n",
    "    G = np.max(np.abs(data - mean)) / std\n",
    "    \n",
    "    # Calculate p-value using t-distribution\n",
    "    t_stat = G * np.sqrt(n-2) / np.sqrt(n-1-G**2)\n",
    "    p_value = 2 * (1 - t.cdf(abs(t_stat), n-2))\n",
    "    \n",
    "    return G, p_value\n",
    "\n",
    "\n",
    "def run_grubbs_test():\n",
    "    # Print skewness\n",
    "    for col in numeric_cols:\n",
    "        print(f\"{col}: Skewness: {df_NSME1988_dirty[col].skew():.2f}\")\n",
    "\n",
    "    print()\n",
    "    print('----------------------------------------------------------')\n",
    "    print()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Original skewness\n",
    "        orig_skew = df_NSME1988_dirty[col].skew()\n",
    "        print(f\"Original skewness: {orig_skew:.2f}\")\n",
    "        \n",
    "        # Log transform (log1p handles zeros)\n",
    "        log_data = np.log1p(df_NSME1988_dirty[col].dropna())\n",
    "        log_skew = stats.skew(log_data)\n",
    "        print(f\"Log-transformed skewness: {log_skew:.2f}\")\n",
    "        \n",
    "        # Now apply Grubbs test on transformed data\n",
    "        if abs(log_skew) < 1:  # Check if transformation worked\n",
    "            G, p_val = grubbs_test(log_data)\n",
    "            print(f\"Grubbs G-statistic: {G:.4f}\")\n",
    "            print(f\"P-value: {p_val:.4f}\")\n",
    "            \n",
    "            if p_val < 0.05:\n",
    "                print(\"Conclusion: Significant outlier detected (p < 0.05)\")\n",
    "            else:\n",
    "                print(\"Conclusion: No significant outlier (p ≥ 0.05)\")\n",
    "        else:\n",
    "            print(\"Still too skewed even after log transformation\")\n",
    "\n",
    "\n",
    "run_grubbs_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042055bd",
   "metadata": {},
   "source": [
    "##### Prove/Disprove which numeric columns contain outliers ######\n",
    "    We want to provide some statisical analysis that proves whether the data truly contains extreme outliers (data errors) or the values are\n",
    "    are within within legitmate ranges.\n",
    "\n",
    "    The data is heavily right skewed. So we can't directly apply Z-Score, so I did some research and found a custom implementation of grubbs_test()\n",
    "    which returns the max deviation from the mean and a p_value.\n",
    "\n",
    "    Data is too skewed, so we first applied log transformation in order to make the distribution more normal. \n",
    "    This stil didn't work particularly well as all the columns (except for chronic) were too skewed.\n",
    "    For 'chronic' we got a P-Value of 0.0088 (p < 0.05) indicating it detected an outlier.\n",
    "\n",
    "    To try to overcome this, I will try additional (Power) transformations:\n",
    "        - Square root transformation\n",
    "        - Yeo-Johnson transformation \n",
    "        - Box-Cox transformation  - Since we have negative values we won't use Box-Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42351f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results in a data frame\n",
    "results = pd.DataFrame(columns=['Original', 'Square Root', 'Yeo-Johnson'])\n",
    "\n",
    "for col in numeric_cols:\n",
    "    \n",
    "    original = df_NSME1988_dirty[col].dropna()\n",
    "    \n",
    "    # Apply transformations\n",
    "    sqrt_data = np.sqrt(original)\n",
    "    yeojohnson_data, _ = yeojohnson(original)\n",
    "    \n",
    "    # Store skewness values\n",
    "    results.loc[col] = [\n",
    "        original.skew(),\n",
    "        sqrt_data.skew(),\n",
    "        pd.Series(yeojohnson_data).skew()\n",
    "    ]\n",
    "\n",
    "print('Results of applying Power Transforms')\n",
    "print('-----------------------------------------')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926f190",
   "metadata": {},
   "source": [
    "##### Analysis of Applied Transformations #####\n",
    "    Based on the results of applying sqrt and Yeo-Johnson:\n",
    "\n",
    "                    Original   Square Root  Yeo-Johnson\n",
    "        visits      3.611181     0.616842    -0.013211\n",
    "        nvisits     7.538863     2.533991    -0.417325\n",
    "        ovisits    19.561863     3.759981    -1.306321\n",
    "        novisits   23.646347     5.781900    -1.614963\n",
    "        emergency   5.040876     2.066182     1.638413\n",
    "        hospital    3.971046     1.956877     1.525857\n",
    "        chronic     1.012530    -0.321434    -0.012175\n",
    "        age         7.413582     6.466510     0.000000\n",
    "        school     -0.453236    -1.905147    -0.109121\n",
    "        income      5.813436     1.787623    -0.098247\n",
    "\n",
    "    It looks like Yeo-Johnson transformation provided the best results.\n",
    "    The p-values look much more reasonable.\n",
    "\n",
    "##### Plot the results of Yeo-Johnson Transformation #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3eb2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Plot original and  Yeo-Johnson transformation to all the numeric columns\n",
    "#\n",
    "\n",
    "\n",
    "n_rows = len(numeric_cols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=2, figsize=(15, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plt.suptitle('Original vs Yeo-Johnson Transformations', fontweight='bold', fontsize=16)\n",
    "\n",
    "# Iterate over each numeric column\n",
    "plot_idx = 0\n",
    "for col in numeric_cols:\n",
    "    # Original data\n",
    "    original = df_NSME1988_dirty[col].dropna()\n",
    "    \n",
    "    # ORIGINAL PLOT\n",
    "    sns.histplot(data=df_NSME1988_dirty, x=col, ax=axes[plot_idx], kde=True)    \n",
    "    axes[plot_idx].set_title(f'{col} - Original', fontweight='bold')\n",
    "\n",
    "    # Calculate statistics for original\n",
    "    mean_val = original.mean()\n",
    "    median_val = original.median()\n",
    "    mode_val = original.mode()[0] if not original.mode().empty else None\n",
    "    std_val = original.std()\n",
    "\n",
    "    # Plot Statistics as Vertical Lines\n",
    "    axes[plot_idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[plot_idx].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "    if mode_val is not None:\n",
    "        axes[plot_idx].axvline(mode_val, color='blue', linestyle='--', linewidth=2, label=f'Mode: {mode_val:.2f}')\n",
    "    \n",
    "    # Add std as shaded region around mean\n",
    "    axes[plot_idx].axvspan(mean_val - std_val, mean_val + std_val, alpha=0.2, color='gray', label=f'Std: {std_val:.2f}')\n",
    "    axes[plot_idx].legend(fontsize=8, loc='upper right')\n",
    "    \n",
    "    plot_idx += 1\n",
    "    \n",
    "    # YEO-JOHNSON TRANSFORMED PLOT\n",
    "    yeojohnson_data, yj_lambda = yeojohnson(original)\n",
    "    yj_series = pd.Series(yeojohnson_data)\n",
    "    \n",
    "    sns.histplot(yj_series, ax=axes[plot_idx], kde=True, color='red')\n",
    "    axes[plot_idx].set_title(f'{col} - Yeo-Johnson (λ={yj_lambda:.2f}, skew={yj_series.skew():.2f})', fontweight='bold')\n",
    "    \n",
    "    # Calculate statistics for transformed\n",
    "    mean_yj = yj_series.mean()\n",
    "    median_yj = yj_series.median()\n",
    "    mode_yj = yj_series.mode()[0] if not yj_series.mode().empty else None\n",
    "    std_yj = yj_series.std()\n",
    "    \n",
    "    # Plot Statistics as Vertical Lines\n",
    "    axes[plot_idx].axvline(mean_yj, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_yj:.2f}')\n",
    "    axes[plot_idx].axvline(median_yj, color='green', linestyle='--', linewidth=2, label=f'Median: {median_yj:.2f}')\n",
    "    if mode_yj is not None:\n",
    "        axes[plot_idx].axvline(mode_yj, color='blue', linestyle='--', linewidth=2, label=f'Mode: {mode_yj:.2f}')\n",
    "    \n",
    "    # Add std as shaded region around mean\n",
    "    axes[plot_idx].axvspan(mean_yj - std_yj, mean_yj + std_yj, alpha=0.2, color='gray', label=f'Std: {std_yj:.2f}')\n",
    "    axes[plot_idx].legend(fontsize=8, loc='upper right')\n",
    "    \n",
    "    plot_idx += 1\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(plot_idx, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "# Make plots more readable and add space\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b152f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# grubbs_test for transformed data\n",
    "#\n",
    "def run_grubbs_test_for_transformed_data():\n",
    "\n",
    "    # Run Grubbs test on transformed data\n",
    "    for col in numeric_cols:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get original data\n",
    "        original = df_NSME1988_dirty[col].dropna()\n",
    "        \n",
    "        # Apply Yeo-Johnson transformation\n",
    "        yeojohnson_data, yj_lambda = yeojohnson(original)\n",
    "        yj_series = pd.Series(yeojohnson_data)\n",
    "        \n",
    "        # Check if transformation improved normality\n",
    "        orig_skew = original.skew()\n",
    "        yj_skew = yj_series.skew()\n",
    "        \n",
    "        print(f\"Original skewness: {orig_skew:.2f}\")\n",
    "        print(f\"Transformed skewness: {yj_skew:.2f}\")\n",
    "        print(f\"Lambda: {yj_lambda:.2f}\")\n",
    "        \n",
    "        # Run Grubbs test on transformed data\n",
    "        if abs(yj_skew) < 1:  # Reasonably normal\n",
    "            G, p_val = grubbs_test(yj_series)\n",
    "            print(f\"\\nGrubbs Test Results:\")\n",
    "            print(f\"  G-statistic: {G:.4f}\")\n",
    "            print(f\"  P-value: {p_val:.4f}\")\n",
    "            \n",
    "            if p_val < 0.05:\n",
    "                print(f\"  ✓ Significant outlier detected (p < 0.05)\")\n",
    "            else:\n",
    "                print(f\"  ✗ No significant outlier detected (p ≥ 0.05)\")\n",
    "        else:\n",
    "            print(f\"\\n⚠ Still too skewed (|{yj_skew:.2f}| >= 1) for reliable parametric test\")\n",
    "\n",
    "\n",
    "#\n",
    "# Rerun Initial grubbs_test\n",
    "#\n",
    "\n",
    "run_grubbs_test_for_transformed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5158c",
   "metadata": {},
   "source": [
    "## <u>**2. Data Wrangling**</u> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053be7a6",
   "metadata": {},
   "source": [
    "### Clean Up Missing Data Issues ###\n",
    "    In our case we've previously identified that the columns with missing data are of type float.\n",
    "    We want to determine which data replacement strategy works best with our data.\n",
    "    We have the following replacement techniques available to us:\n",
    "- **Mean imputation**: Replace missing values with the column mean\n",
    "- **Median imputation**: Replace missing values with the column median\n",
    "- **KNN imputation**: Use K-Nearest Neighbors to estimate missing values\n",
    "- **Iterative imputation**: Model each feature as a function of others (MICE algorithm)\n",
    "\n",
    "## ##\n",
    "    What we really want to do is apply the different techniques and then determine which one works best for our needs.\n",
    "    We could do this 2 ways:\n",
    "- **Apply the same method for the desired columns** </br>\n",
    "(Simpler/Faster)\n",
    "- **Iterate through the columns one by one to determine what is best for each column** </br>\n",
    "(More Precise, but doesn't make sense for all strategies - KNN/Mice use other columns)\n",
    "\n",
    "## ##\n",
    "    For expediency and simplification purposes, we will use the same missing data replacement technique across columns.\n",
    "\n",
    "    To find the best technique, we must test against known data. \n",
    "    To that end, we'll create a sample data set void of rows with missing data.\n",
    "    We'll make a copy of that data set (A). Next we'll making a second data set A and randomly\n",
    "    create missing values across the columsn of interest.\n",
    "    Then we will run our selected replacement strategies capturing the results (with missing data replaced).\n",
    "    We then compare our original sample (A) against the modified samples and determine which technique works best.\n",
    "\n",
    "\n",
    "##### Filter out existing rows with missing data - create dataset copy with manufactured missing rows#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Count the number of \"complete/clean\" rows of data and number of rows that have missing data\n",
    "#\n",
    "\n",
    "columns_with_missing_data = ['visits','nvisits','ovisits','novisits','emergency',\n",
    "                            'hospital','chronic','age','school','income']\n",
    "complete_rows = df_NSME1988_dirty[columns_with_missing_data].dropna()\n",
    "rows_with_missing_data = df_NSME1988_dirty[df_NSME1988_dirty[columns_with_missing_data].isnull().any(axis=1)]\n",
    "\n",
    "print(f\"Complete rows: {len(complete_rows)}\")\n",
    "print(f\"Rows with missing data: {len(rows_with_missing_data)}\")\n",
    "\n",
    "\n",
    "# Create artificial missingness\n",
    "test_data = complete_rows.copy()\n",
    "np.random.seed(42)\n",
    "missing_mask = np.random.random(test_data.shape) < 0.10\n",
    "true_values = test_data.values[missing_mask]\n",
    "test_data.values[missing_mask] = np.nan\n",
    "\n",
    "# Test all methods\n",
    "results = {}\n",
    "\n",
    "print(df_NSME1988_dirty.head(20))\n",
    "\n",
    "# Mean (all columns)\n",
    "mean_imp = SimpleImputer(strategy='mean')\n",
    "results['Mean'] = mean_absolute_error(true_values, mean_imp.fit_transform(test_data)[missing_mask])\n",
    "\n",
    "# Median (all columns)\n",
    "median_imp = SimpleImputer(strategy='median')\n",
    "results['Median'] = mean_absolute_error(true_values, median_imp.fit_transform(test_data)[missing_mask])\n",
    "\n",
    "# KNN (uses relationships between columns)\n",
    "knn_imp = KNNImputer(n_neighbors=5)\n",
    "results['KNN'] = mean_absolute_error(true_values, knn_imp.fit_transform(test_data)[missing_mask])\n",
    "\n",
    "# MICE (uses relationships between columns)\n",
    "mice_imp = IterativeImputer(random_state=42)\n",
    "results['MICE'] = mean_absolute_error(true_values, mice_imp.fit_transform(test_data)[missing_mask])\n",
    "\n",
    "# Winner\n",
    "print('---------------------------------------------------------')\n",
    "print('Imputation Results')\n",
    "print(results)\n",
    "print('--------------------------------------------------------')\n",
    "best = min(results, key=results.get)\n",
    "print(f\"Best method: {best}\")\n",
    "\n",
    "#\n",
    "# For additional analysis, let's run the imputations and plot results against originals\n",
    "#\n",
    "\n",
    "df_mean = df_NSME1988_dirty.copy()\n",
    "df_median = df_NSME1988_dirty.copy()\n",
    "\n",
    "for col in columns_with_missing_data:\n",
    "\n",
    "    # Mean Imputation\n",
    "    df_mean.fillna({col: df_mean[col].mean()}, inplace=True)\n",
    "\n",
    "    # Median Imputation\n",
    "    df_median.fillna({col: df_median[col].median()}, inplace=True)\n",
    "\n",
    "# KNN Imputation (all columns at once)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "knn_array = knn_imputer.fit_transform(df_NSME1988_dirty[columns_with_missing_data])\n",
    "\n",
    "# Convert entire result to DataFrame\n",
    "df_knn = pd.DataFrame(\n",
    "    knn_array,\n",
    "    columns=columns_with_missing_data,\n",
    "    index=df_NSME1988_dirty.index\n",
    ")    \n",
    "\n",
    "# Iterative Imputation\n",
    "iterative_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "iterative_array = iterative_imputer.fit_transform(df_NSME1988_dirty[numeric_cols])\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_iterative = pd.DataFrame(\n",
    "    iterative_array,\n",
    "    columns=numeric_cols,\n",
    "    index=df_NSME1988_dirty.index\n",
    ")\n",
    "\n",
    "\n",
    "n_rows = len(columns_with_missing_data)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=4, figsize=(15, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define imputation methods and their properties\n",
    "imputation_methods = [\n",
    "    {'data': df_mean, 'title': 'Mean Imputation', 'color': 'green', 'label': 'Mean Imputed'},\n",
    "    {'data': df_median, 'title': 'Median Imputation', 'color': 'orange', 'label': 'Median Imputed'},\n",
    "    {'data': df_knn, 'title': 'KNN Imputation', 'color': 'purple', 'label': 'KNN Imputed'},\n",
    "    {'data': df_iterative, 'title': 'Iterative Imputation', 'color': 'teal', 'label': 'Iterative Imputed'}\n",
    "]\n",
    "\n",
    "pltIdx = 0\n",
    "\n",
    "for col in columns_with_missing_data:\n",
    "\n",
    "    # Create KDE of original distribution\n",
    "    kde_original = stats.gaussian_kde(df_NSME1988_dirty[col].dropna())\n",
    "    x_range = np.linspace(df_NSME1988_dirty[col].min(), df_NSME1988_dirty[col].max(), 200)\n",
    "    kde_values = kde_original(x_range)\n",
    "\n",
    "    # Plot each imputation method\n",
    "    for method in imputation_methods:\n",
    "\n",
    "        ax = axes[pltIdx]\n",
    "        ax.set_title(method['title'])\n",
    "\n",
    "        ax.hist(\n",
    "            method['data'][col],\n",
    "            bins=30, density=True,\n",
    "            edgecolor='black', color=method['color'],\n",
    "            label=method['label']\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            x_range, kde_values,\n",
    "            'b-', linewidth=2.5,\n",
    "            label='Original Distribution'\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "        pltIdx += 1\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(plot_idx, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "# Make plots more readable and add space\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1ee0c",
   "metadata": {},
   "source": [
    "#### Imputation Analysis Results ####\n",
    "    Basing decision on visual analysis of Imputation vs Original plots is difficult due to the skewedness of the data.\n",
    "    We have to rely on Mean Absolute Error (MAE) calculations for each of the Imputation types\n",
    "    \n",
    "**Imputation Results** </br>\n",
    "        Median: 1.4677338374016535 </br>\n",
    "        MICE:   1.6830630938567108</br>\n",
    "        KNN:    1.680683066185563 </br>\n",
    "        Mean:   1.7257226749105237 </br>\n",
    "\n",
    "**Median Imputation** had the lowest MAE and is the clear winner.\n",
    "## ##\n",
    "    Apply Median Imputation to the numeric columns with missing data.\n",
    "    Then test to see if we have any remaining columns with missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Replace numeric columns with missing data in the original data set with imputated values\n",
    "#\n",
    "df_NSME1988_dirty_new[columns_with_missing_data] = df_median[columns_with_missing_data]\n",
    "\n",
    "# See if we have any remaining columns with missing data\n",
    "# Verify no missing values remain\n",
    "missing_data_column_count = df_NSME1988_dirty_new[columns_with_missing_data].isnull().sum().sum()\n",
    "print(f'Number of columns with missing data: {missing_data_column_count}')\n",
    "\n",
    "if missing_data_column_count > 0:\n",
    "    print('We still have the following columns with missing data:')\n",
    "    print(df_NSME1988_dirty_new.isnull().sum()[df_NSME1988_dirty_new.isnull().sum() > 0])\n",
    "else:\n",
    "    print('We have cleaned up all the missing data in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0f4b8",
   "metadata": {},
   "source": [
    "### Handle Negative Values ###\n",
    "    In our data set, negative values are not valid. \n",
    "    They are more than likely data errors.\n",
    "    The best way to handle them is to replace them with zeros (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922644fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Replace negative values with zeros\n",
    "#\n",
    "\n",
    "cols_with_negatives = [col for col in numeric_cols if (df_NSME1988_dirty_new[col] < 0).sum() > 0]\n",
    "print(f\"\\nTotal columns with negative values: {len(cols_with_negatives)}\")\n",
    "print(f'Columns with Negative Values are {cols_with_negatives}')\n",
    "\n",
    "for col in numeric_cols:\n",
    "    negative_count = (df_NSME1988_dirty_new[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        print(f\"{col}: {negative_count} negative values found, replacing with 0\")\n",
    "        df_NSME1988_dirty_new[col] = df_NSME1988_dirty_new[col].clip(lower=0)\n",
    "\n",
    "print()\n",
    "print('Normalization of negative values completed')\n",
    "cols_with_negatives = [col for col in numeric_cols if (df_NSME1988_dirty_new[col] < 0).sum() > 0]\n",
    "print(f\"\\nTotal columns with negative values: {len(cols_with_negatives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310d8d5",
   "metadata": {},
   "source": [
    "### \"Normalize\" Age and Income Columns to have realistic values\n",
    "    After some investigation and thought about 'age' and 'income', I was able to come to some assumptions.\n",
    "    If I multiply 'age' by a factor of 10, I get what look like some reasonable numbers.\n",
    "    Income was a little harder to figure out. After trying different strategies and looking at the resulting ranges,\n",
    "    to me it looks like income is really 'monthly income', not 'yearly income'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22fe432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mulitply Age by 10 to get true age in years\n",
    "df_NSME1988_dirty_new['age'] = df_NSME1988_dirty_new['age'] * 10\n",
    "\n",
    "# Mulitply Income by 1000 to get true value\n",
    "df_NSME1988_dirty_new['income'] = df_NSME1988_dirty_new['income'] * 1000\n",
    "\n",
    "print('------------------Modified Data--------------------')\n",
    "print(df_NSME1988_dirty_new[['age','income']].describe().T)\n",
    "print('--------------------------------------')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fda2b",
   "metadata": {},
   "source": [
    "### Reanalyze and ID and Examine Potential Outliers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627378d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replot Histograms and BoxPlots for dataframe with corrected Missing Values\n",
    "\n",
    "PlotHistogramsToViewDistributions(df_NSME1988_dirty_new)\n",
    "\n",
    "PlotBoxPlotsToViewDistributions(df_NSME1988_dirty_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9bf79",
   "metadata": {},
   "source": [
    "#### We still see Right Skewed Distrbutions with Extreme Outliers ####\n",
    "    The next step is to analytically quanitfy the existance of outliers on the numeric columns\n",
    "    We will use the following methods, then visualize the results across the different column types:\n",
    "- **IQR method**: Use interquartile range (Q3 - Q1) to identify outliers\n",
    "- **Z-score method**: Identify values more than 3 standard deviations from the mean\n",
    "- **Isolation Forest**: Machine learning algorithm that isolates anomalies\n",
    "- **Local Outlier Factor (LOF)**: Density-based method comparing local density to neighbors    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac73dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(data, column, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using the IQR method.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame\n",
    "    - column: column name to check for outliers\n",
    "    - multiplier: IQR multiplier (typically 1.5 for outliers, 3.0 for extreme outliers)\n",
    "    \n",
    "    Returns:\n",
    "    - Boolean series indicating outliers\n",
    "    \"\"\"\n",
    "\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Sample Call:\n",
    "# outliers_iqr = detect_outliers_iqr(df_original, outlier_feature)\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Z-Score method.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame\n",
    "    - column: column name to check for outliers\n",
    "    - threshold: z-score threshold (typically 3)\n",
    "    \n",
    "    Returns:\n",
    "    - Boolean series indicating outliers\n",
    "    \"\"\"\n",
    "\n",
    "    mean = data[column].mean()\n",
    "    std = data[column].std()\n",
    "    z_scores = np.abs((data[column] - mean) / std)\n",
    "    \n",
    "    outliers = z_scores > threshold\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Sample Call\n",
    "#outliers_zscore = detect_outliers_zscore(df_original, outlier_feature)\n",
    "\n",
    "def detect_outliers_isolation_forest(data, column, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detect outliers using Isolation Forest.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame\n",
    "    - column: column name to check for outliers\n",
    "    - contamination: expected proportion of outliers (default 0.05 = 5%)\n",
    "    \n",
    "    Returns:\n",
    "    - Boolean series indicating outliers\n",
    "    \"\"\"\n",
    "    # Isolation Forest requires 2D array\n",
    "    X = data[[column]].values\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "    predictions = iso_forest.fit_predict(X)\n",
    "    \n",
    "    # Isolation Forest returns -1 for outliers, 1 for inliers\n",
    "    outliers = predictions == -1\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Sample Call\n",
    "#outliers_iforest = detect_outliers_isolation_forest(df_original, outlier_feature)\n",
    "\n",
    "def detect_outliers_lof(data, column, n_neighbors=1000, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detect outliers using Local Outlier Factor.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame\n",
    "    - column: column name to check for outliers\n",
    "    - n_neighbors: number of neighbors to consider (default 1000)\n",
    "    \n",
    "    Returns:\n",
    "    - Boolean series indicating outliers\n",
    "    \"\"\"\n",
    "\n",
    "    # LOF requires 2D array\n",
    "    X = data[[column]].values\n",
    "    \n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "    predictions = lof.fit_predict(X)\n",
    "    \n",
    "    # LOF returns -1 for outliers, 1 for inliers\n",
    "    outliers = predictions == -1\n",
    "\n",
    "    return outliers\n",
    "\n",
    "# Sample Call\n",
    "# outliers_lof = detect_outliers_lof(df_original, outlier_feature)\n",
    "\n",
    "# Iterate over all the numeric columns and gather/store Outlier information across our 4 methods\n",
    "\n",
    "outlier_results = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "\n",
    "    outliers_iqr = detect_outliers_iqr(df_NSME1988_dirty_new, col)\n",
    "    outliers_zscore = detect_outliers_zscore(df_NSME1988_dirty_new, col)\n",
    "    outliers_iforest = detect_outliers_isolation_forest(df_NSME1988_dirty_new, col)\n",
    "    outliers_lof = detect_outliers_lof(df_NSME1988_dirty_new, col)\n",
    "\n",
    "    outlier_results.append({'column_name':col, \n",
    "                            'IQR': outliers_iqr,\n",
    "                            'Z-Score': outliers_zscore,\n",
    "                            'IForest':outliers_iforest,\n",
    "                            'LOF': outliers_lof} )\n",
    "\n",
    "print('----------------Outlier Results-----------------')\n",
    "print(outlier_results)    \n",
    "print(f'Outlier Length: {len(outlier_results)}')\n",
    "print('---------------------------------')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19532587",
   "metadata": {},
   "source": [
    "#### Plot the Outlier Detection Results for Comparison ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6704832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Plot the Outlier Test Results\n",
    "#\n",
    "\n",
    "colIdx=0\n",
    "for col in numeric_cols:\n",
    "    \n",
    "    # Detect outliers for this column\n",
    "    outliers_iqr = outlier_results[colIdx]['IQR']\n",
    "    outliers_zscore = outlier_results[colIdx]['Z-Score']\n",
    "    outliers_iforest = outlier_results[colIdx]['IForest']\n",
    "    outliers_lof = outlier_results[colIdx]['LOF']\n",
    "    colIdx += 1\n",
    "    \n",
    "    # Precompute bins\n",
    "    hist_bins = np.linspace(df_NSME1988_dirty_new[col].min(), df_NSME1988_dirty_new[col].max(), 31)\n",
    "    \n",
    "    # Create figure for THIS column\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    fig.suptitle(f'Outlier Detection: {col}', fontsize=16, fontweight='bold')\n",
    "    gs = GridSpec(4, 2, figure=fig, width_ratios=[4, 1], hspace=0.4, wspace=0.05)\n",
    "    \n",
    "    detection_methods = [\n",
    "        {'outliers': outliers_iqr, 'title': 'IQR Method'},\n",
    "        {'outliers': outliers_zscore, 'title': 'Z-Score Method'},\n",
    "        {'outliers': outliers_iforest, 'title': 'Isolation Forest'},\n",
    "        {'outliers': outliers_lof, 'title': 'Local Outlier Factor'}\n",
    "    ]\n",
    "    \n",
    "    for idx, method in enumerate(detection_methods):\n",
    "        outliers = method['outliers']\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax_scatter = fig.add_subplot(gs[idx, 0])\n",
    "        ax_scatter.set_title(f\"{method['title']}: {outliers.sum()} outliers detected\")\n",
    "        \n",
    "        ax_scatter.scatter(\n",
    "            df_NSME1988_dirty_new.index[~outliers], \n",
    "            df_NSME1988_dirty_new.loc[~outliers, col],\n",
    "            c='teal', alpha=0.5, s=5\n",
    "        )\n",
    "        ax_scatter.scatter(\n",
    "            df_NSME1988_dirty_new.index[outliers], \n",
    "            df_NSME1988_dirty_new.loc[outliers, col],\n",
    "            c='orange', alpha=0.7, s=5, label='Outliers'\n",
    "        )\n",
    "        ax_scatter.legend(loc='lower left', framealpha=1, markerscale=3)\n",
    "        \n",
    "        # Histogram\n",
    "        ax_hist = fig.add_subplot(gs[idx, 1], sharey=ax_scatter)\n",
    "        ax_hist.hist(df_NSME1988_dirty_new.loc[~outliers, col], \n",
    "                     bins=hist_bins, \n",
    "                     orientation='horizontal', color='teal', \n",
    "                     alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        ax_hist.hist(df_NSME1988_dirty_new.loc[outliers, col], \n",
    "                     bins=hist_bins,\n",
    "                     orientation='horizontal', color='orange', \n",
    "                     edgecolor='black')\n",
    "        ax_hist.tick_params(labelleft=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")  # Visual break between columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a319dc4",
   "metadata": {},
   "source": [
    "#### Examine Binning of Income ####\n",
    "    I want to view the income feature binned to see the distribution clearer and eliminate effect of outliers/noise\n",
    "    I only want to observe this and not include it in my modified datafraem. I want to work with continious variable for income there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Bin Income\n",
    "#\n",
    "\n",
    "# Create temporary dataframe\n",
    "df_income = df_NSME1988_dirty_new.copy()\n",
    "\n",
    "# Bin income into 5 categories\n",
    "df_income['income_group'] = pd.cut(\n",
    "                                    df_income['income'],\n",
    "                                    bins=5,\n",
    "                                    labels=['Low', 'Low-Middle', 'Middle', 'Middle-High', 'High']\n",
    "                                )\n",
    "\n",
    "# Check the distribution\n",
    "print(df_income['income_group'].value_counts())\n",
    "\n",
    "# Plot countplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_income, x='income_group', \n",
    "              order=['Low', 'Low-Middle', 'Middle', 'Middle-High', 'High'],\n",
    "              palette='viridis')\n",
    "plt.title('Income Distribution by Category', fontweight='bold')\n",
    "plt.xlabel('Income Group')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d38c4",
   "metadata": {},
   "source": [
    "#### Income Binning Analysis ####\n",
    "    After binning income into appropriate bins and displaying results in a count plot,\n",
    "    we see the data definitely has a strong Right Skew. Most people fall into the \"Low\" income category.\n",
    "    This is probably due to the nature of sample set (older population) - fixed income, retired.\n",
    "    We don't want to lose the data for the higher income levels because it could definitely be an indicator of overall health."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdebb2b",
   "metadata": {},
   "source": [
    "#### Additional Adjustments - Age ####\n",
    "    Age looks like it has data errors - no one is 400 years old.\n",
    "    To resolve, decide on a reasonable max and adjust anything above it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_valid_age = 110\n",
    "\n",
    "# Calculate median of valid ages only\n",
    "valid_median = df_NSME1988_dirty_new.loc[df_NSME1988_dirty_new['age'] <= max_valid_age, 'age'].median()\n",
    "df_NSME1988_dirty_new.loc[df_NSME1988_dirty_new['age'] > max_valid_age, 'age'] = valid_median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e77ff1",
   "metadata": {},
   "source": [
    "#### Outlier Detection Analysis ####\n",
    "    Age, Education and Chronic - it doesn't make sense to cap() or remove as they seem like they could be valid indicators\n",
    "    Apply Winsorization (capping) to the remaining columns.\n",
    "    After further examination, income still seems to be a bit of a mystery. \n",
    "        - What is the real multiplication factor?\n",
    "        - Should I really be working to make it an absolute dollar value for readability?\n",
    "        - Should I really be capping it? Is binning more appropriate?\n",
    "    It feels like if I were to continue to work on this dataset, I'd continue to tinker with the income feature (or not).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedd516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cap the data\n",
    "#\n",
    "\n",
    "# Cap at 5th and 95th percentiles\n",
    "def cap_outliers(data, column, lower_pct=0.05, upper_pct=0.95):\n",
    "    lower = data[column].quantile(lower_pct)\n",
    "    upper = data[column].quantile(upper_pct)\n",
    "    data[column] = data[column].clip(lower, upper)\n",
    "    return data\n",
    "\n",
    "columns_to_be_capped = ['visits', 'nvisits', 'ovisits', 'novisits', 'emergency', 'hospital', 'income']\n",
    "\n",
    "print('------------------Before Modificaton--------------------')\n",
    "print(df_NSME1988_dirty_new[['age','income']].describe().T)\n",
    "print('--------------------------------------')\n",
    "print()\n",
    "\n",
    "for col in columns_to_be_capped:\n",
    "    df_NSME1988_dirty_new = cap_outliers(df_NSME1988_dirty_new.copy(), col)\n",
    "\n",
    "print('------------------Modified Data--------------------')\n",
    "print(df_NSME1988_dirty_new[['age','income']].describe().T)\n",
    "print('--------------------------------------')\n",
    "print()\n",
    "\n",
    "\n",
    "print('Plot to see if changes affected distributions')\n",
    "PlotHistogramsToViewDistributions(df_NSME1988_dirty_new)\n",
    "PlotBoxPlotsToViewDistributions(df_NSME1988_dirty_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb5fa0",
   "metadata": {},
   "source": [
    "#### Re-examine Descriptive Statistics of Modifed Dataset ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DisplayDescriptiveStats(dataset, columns_to_display=None):\n",
    "    # Print Stats\n",
    "    print(\"***********************************\")\n",
    "    print(\"Description Stats of Modified Dataset\")\n",
    "    print(\"***********************************\")\n",
    "    print()\n",
    "    if columns_to_display is None:\n",
    "        print(dataset.describe(include='all').T)\n",
    "    else:\n",
    "        print(dataset[columns_to_display].describe().T)\n",
    "    print()\n",
    "\n",
    "    # Print df Column Info\n",
    "    print(\"***********************************\")\n",
    "    print(\"Basic Info of Modified DataSet\")\n",
    "    print(\"***********************************\")\n",
    "    print()\n",
    "    dataset.info()\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print(\"***********************************\")\n",
    "    print(\"First 20 rows of Data\")\n",
    "    print(\"***********************************\")\n",
    "    print()\n",
    "    print(dataset.head(20))\n",
    "    print()\n",
    "\n",
    "\n",
    "DisplayDescriptiveStats(df_NSME1988_dirty_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b842edc",
   "metadata": {},
   "source": [
    "## <u>**3. Feature Engineering**</u> ##\n",
    "### Feature Encoding ###\n",
    "#### One-Hot Encoding ####\n",
    "    Several features - Health and Region - Models like working with numbers\n",
    "    Gender - currently only 2 choices (based on dataset), but make it One-Hot encoded should it change\n",
    "#### Ordinal Encoding ####\n",
    "    ADL is really what we are trying to predict with this model. \n",
    "    It doesn't make sense to Hot-Encode, but it does to Ordinal Encode it.\n",
    "    That way we'll be able tp plot/observe correlations between our dependent/independent variables.\n",
    "    (Models like working with numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# One-Hot Encoding\n",
    "#\n",
    "\n",
    "# Use the nominal categorical variable (region)\n",
    "# Apply one-hot encoding\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "# Convert Region\n",
    "onehot_array = encoder.fit_transform(df_NSME1988_dirty_new[['region']])\n",
    "\n",
    "# Create dataframe with proper column names\n",
    "df_onehot = pd.DataFrame(\n",
    "    onehot_array,\n",
    "    columns=encoder.get_feature_names_out(['region']),\n",
    "    index=df_NSME1988_dirty_new.index\n",
    ")\n",
    "\n",
    "# Join back to original and drop original column\n",
    "df_NSME1988_dirty_new = pd.concat([df_NSME1988_dirty_new, df_onehot], axis=1)\n",
    "\n",
    "# Don't drop region so we can use it when we create our multi column features\n",
    "# df_NSME1988_dirty_new = df_NSME1988_dirty_new.drop('region', axis=1)\n",
    "\n",
    "# gender\n",
    "onehot_array = encoder.fit_transform(df_NSME1988_dirty_new[['gender']])\n",
    "\n",
    "# Create dataframe with proper column names\n",
    "df_onehot = pd.DataFrame(\n",
    "    onehot_array,\n",
    "    columns=encoder.get_feature_names_out(['gender']),\n",
    "    index=df_NSME1988_dirty_new.index\n",
    ")\n",
    "\n",
    "# Join back to original and drop original column\n",
    "df_NSME1988_dirty_new = pd.concat([df_NSME1988_dirty_new, df_onehot], axis=1)\n",
    "df_NSME1988_dirty_new = df_NSME1988_dirty_new.drop('gender', axis=1)\n",
    "\n",
    "# adl\n",
    "print(f'ADL Values:{df_NSME1988_dirty_new['adl'].unique()}')\n",
    "\n",
    "# Apply ordinal encoding for adl\n",
    "categories = [['limited', 'normal']]\n",
    "encoder = OrdinalEncoder(categories=categories)\n",
    "df_NSME1988_dirty_new['adl'] = encoder.fit_transform(df_NSME1988_dirty_new[['adl']])\n",
    "\n",
    "# health\n",
    "print(f'Health Values:{df_NSME1988_dirty_new['health'].unique()}')\n",
    "\n",
    "# Apply ordinal encoding for adl\n",
    "categories = [['poor', 'average', 'excellent']]\n",
    "encoder = OrdinalEncoder(categories=categories)\n",
    "df_NSME1988_dirty_new['health'] = encoder.fit_transform(df_NSME1988_dirty_new[['health']])\n",
    "\n",
    "# Stats for Altered DataFrame\n",
    "print('DataFrame with One-Hot and Ordinal Encoding Modifications')\n",
    "DisplayDescriptiveStats(df_NSME1988_dirty_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3641e3",
   "metadata": {},
   "source": [
    "#### Label Encoding ####\n",
    "    Several features - Employed, Insurance, Medicaid, Married\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc89899",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# Apply to each column\n",
    "df_NSME1988_dirty_new['employed'] = le.fit_transform(df_NSME1988_dirty_new['employed'])\n",
    "df_NSME1988_dirty_new['married'] = le.fit_transform(df_NSME1988_dirty_new['married'])\n",
    "df_NSME1988_dirty_new['insurance'] = le.fit_transform(df_NSME1988_dirty_new['insurance'])\n",
    "df_NSME1988_dirty_new['medicaid'] = le.fit_transform(df_NSME1988_dirty_new['medicaid'])\n",
    "\n",
    "# Stats for Altered DataFrame\n",
    "print('DataFrame with Label Encoding Modifications')\n",
    "DisplayDescriptiveStats(df_NSME1988_dirty_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b5132",
   "metadata": {},
   "source": [
    "#### Data Type Conversion ####\n",
    "    After looking over all the numeric columns (including the new ones generated through encoding),\n",
    "    I was able to convert all the existing floating point variables into integers.\n",
    "    Given their lower value ranges, a majority of them were convertered to int8.\n",
    "    Income was converted to int32 to accomodate it's high ceiling. For our analysis purposed,\n",
    "    we don't really care about \"cents\" values, whole dollars is more appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d348224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Int64 to Int16 conversions\n",
    "int8_conversion_columns = [ 'visits','nvisits','ovisits','novisits','emergency','hospital','chronic','age', \n",
    "                            'school','region_northeast','region_other','region_west','gender_male']\n",
    "\n",
    "int32_conversion_columns = [ 'income']\n",
    "\n",
    "df_NSME1988_dirty_new[int8_conversion_columns] = df_NSME1988_dirty_new[int8_conversion_columns].astype('int8')\n",
    "df_NSME1988_dirty_new[int32_conversion_columns] = df_NSME1988_dirty_new[int32_conversion_columns].astype('int32')\n",
    "\n",
    "# Stats for Altered DataFrame\n",
    "print('DataFrame with Data Type Conversions')\n",
    "DisplayDescriptiveStats(df_NSME1988_dirty_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ec5bb",
   "metadata": {},
   "source": [
    "#### Create Correlation Matrix for Feature Analysis ####\n",
    "    Create a correlation matrix to look for relationships between freature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e36b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Examine Correlation\n",
    "#\n",
    "\n",
    "\n",
    "# omit categorical column region\n",
    "corr = df_NSME1988_dirty_new.drop(columns=['region']).corr()\n",
    "# print(type(corr))\n",
    "# print(corr)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Table\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.axis('off')\n",
    "table = plt.table(\n",
    "    cellText=corr.round(2).values,\n",
    "    rowLabels=corr.index,\n",
    "    colLabels=corr.columns,\n",
    "    cellLoc='center',\n",
    "    loc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(7)\n",
    "table.scale(1.2, 1.8)\n",
    "plt.title('Correlation Table', fontweight='bold', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928e830",
   "metadata": {},
   "source": [
    "#### Create Combined Features #####\n",
    "    Create some combined features that make sense given the dataset:\n",
    "        - total vistis - I'm assuming that existing 'visits' is not the total number of visits\n",
    "        so I include it in my total number of visits (needs clarifcation)\n",
    "        - total visits per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Multi feature columns by hand\n",
    "#\n",
    "\n",
    "# Total Visits\n",
    "df_NSME1988_dirty_new['total_visits'] = df_NSME1988_dirty_new['visits'] + df_NSME1988_dirty_new['nvisits'] + df_NSME1988_dirty_new['ovisits'] \n",
    "+ df_NSME1988_dirty_new['novisits'] + df_NSME1988_dirty_new['hospital'] + df_NSME1988_dirty_new['emergency']\n",
    "\n",
    "# Mean by group\n",
    "df_NSME1988_dirty_new['avg_visits_by_region'] = df_NSME1988_dirty_new.groupby('region')['total_visits'].transform('mean')\n",
    "df_NSME1988_dirty_new['avg_hospital_by_region'] = df_NSME1988_dirty_new.groupby('region')['hospital'].transform('mean')\n",
    "df_NSME1988_dirty_new['avg_emergency_by_region'] = df_NSME1988_dirty_new.groupby('region')['emergency'].transform('mean')\n",
    "\n",
    "# Count by group\n",
    "df_NSME1988_dirty_new['patients_per_region'] = df_NSME1988_dirty_new.groupby('region')['total_visits'].transform('count')\n",
    "df_NSME1988_dirty_new['hospital_by_region'] = df_NSME1988_dirty_new.groupby('region')['hospital'].transform('count')\n",
    "df_NSME1988_dirty_new['emergency_by_region'] = df_NSME1988_dirty_new.groupby('region')['emergency'].transform('count')\n",
    "\n",
    "# Older patients with chronic conditions\n",
    "df_NSME1988_dirty_new['age_chronic'] = df_NSME1988_dirty_new['age'] * df_NSME1988_dirty_new['chronic']\n",
    "\n",
    "# Visits per chronic condition\n",
    "df_NSME1988_dirty_new['visits_per_chronic'] = df_NSME1988_dirty_new['total_visits'] / (df_NSME1988_dirty_new['chronic'] + 1)\n",
    "\n",
    "# Hospital visits per year of age\n",
    "df_NSME1988_dirty_new['hospital_by_age'] = df_NSME1988_dirty_new['hospital'] / (df_NSME1988_dirty_new['age'] + 1)\n",
    "\n",
    "# Emergency visits per year of age\n",
    "df_NSME1988_dirty_new['emergency_by_age'] = df_NSME1988_dirty_new['emergency'] / (df_NSME1988_dirty_new['age'] + 1)\n",
    "\n",
    "# Total visits per year of age\n",
    "df_NSME1988_dirty_new['total_visits_by_age'] = df_NSME1988_dirty_new['total_visits'] / (df_NSME1988_dirty_new['age'] + 1)\n",
    "\n",
    "# plot histograms for these new features\n",
    "\n",
    "new_cols = ['total_visits','avg_visits_by_region','avg_hospital_by_region','avg_emergency_by_region',\n",
    "\t        'patients_per_region','hospital_by_region','emergency_by_region', 'age_chronic', 'visits_per_chronic',\n",
    "            'hospital_by_age','emergency_by_age','total_visits_by_age']\n",
    "\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = 4\n",
    "print(f'Number of Columns: {n_cols}')\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plt.suptitle('Feature Distributions of Newly Created Multi-Column Features', fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.90])  # Leave space at top\n",
    "\n",
    "#Iterate over each numeric column and plot a historgram\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.histplot(data=df_NSME1988_dirty_new, x=col, ax=axes[i], kde=True)    \n",
    "    axes[i].set_title(col, fontweight='bold')\n",
    "\n",
    "    # Calculate statistics\n",
    "    mean_val = df_NSME1988_dirty_new[col].mean()\n",
    "    median_val = df_NSME1988_dirty_new[col].median()\n",
    "    mode_val = df_NSME1988_dirty_new[col].mode()[0] if not df_NSME1988_dirty_new[col].mode().empty else None\n",
    "    std_val = df_NSME1988_dirty_new[col].std()\n",
    "\n",
    "    # Plot Statistics as Vertical Lines\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[i].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "    if mode_val is not None:\n",
    "        axes[i].axvline(mode_val, color='blue', linestyle='--', linewidth=2, label=f'Mode: {mode_val:.2f}')\n",
    "    \n",
    "    # Add std as shaded region around mean\n",
    "    axes[i].axvspan(mean_val - std_val, mean_val + std_val, alpha=0.2, color='gray', label=f'Std: {std_val:.2f}')\n",
    "    \n",
    "    # Add legend\n",
    "    axes[i].legend(fontsize=8, loc='upper right')\n",
    "\n",
    "\n",
    "# Make plots more readable and add space\n",
    "plt.tight_layout(h_pad=5)\n",
    "plt.show()   \n",
    "print()\n",
    "print()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plt.suptitle('Newly Created Multi-Column Features Distributions - Box Plots', fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.90])  # Leave space at top\n",
    "\n",
    "#Iterate over each numeric column and plot a historgram\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.boxplot(data=df_NSME1988_dirty_new, y=col, ax=axes[i])    \n",
    "    axes[i].set_title(col, fontweight='bold')\n",
    "\n",
    "    # Optional: Add count of outliers\n",
    "    Q1 = df_NSME1988_dirty_new[col].quantile(0.25)\n",
    "    Q3 = df_NSME1988_dirty_new[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outlier_count = len(df_NSME1988_dirty_new[(df_NSME1988_dirty_new[col] < lower) | \n",
    "                                        (df_NSME1988_dirty_new[col] > upper)])\n",
    "    axes[i].text(0.5, 0.95, f'Outliers: {outlier_count}', \n",
    "                transform=axes[i].transAxes, ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Make plots more readable and add space\n",
    "plt.tight_layout(h_pad=5)\n",
    "plt.show()   \n",
    "\n",
    "# Stats for newly created features\n",
    "print('Stats for Newly Created Features')\n",
    "DisplayDescriptiveStats(df_NSME1988_dirty_new, new_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ad5fe",
   "metadata": {},
   "source": [
    "#### Create Combined Features Through Polynomial Features #####\n",
    "    Initially I thought I could use PolynomialFeatures() to create additional \"combined-feature\" columns.\n",
    "    But upon further investigation, I realized I got confused. PolynomialFeatures() work best when we have \n",
    "    a dependent variable (target) we'are trying predict based on independent variables (remaining features).\n",
    "\n",
    "    Initially (confused) I thought adl was our target (which I coded towards with PolynomialFeatures()).\n",
    "    But adl (and health) are amongst the list of independent features, so I skipped this approach.\n",
    "    To that end, I just stuck with the \"combined-features\" that I created by hand.\n",
    "\n",
    "    Am I still confused/wrong..? Is there an intended target in the dataset that I just missed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30234a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# # Create Polynomial Features\n",
    "# #\n",
    "\n",
    "# # All columns except region\n",
    "# cols_to_combine = [col for col in df_NSME1988_dirty_new.columns \n",
    "#                    if 'region' not in col and col != 'adl']\n",
    "\n",
    "# # Create polynomial features - 3 degrees\n",
    "# poly = PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "# poly_features = poly.fit_transform(df_NSME1988_dirty_new[cols_to_combine])\n",
    "\n",
    "# #  Convert to DataFrame\n",
    "# poly_df = pd.DataFrame(\n",
    "#                         poly_features,\n",
    "#                         columns=poly.get_feature_names_out(cols_to_combine),\n",
    "#                         index=df_NSME1988_dirty_new.index\n",
    "#                     )\n",
    "\n",
    "# # Filter by correlation threshold with target\n",
    "# threshold = 0.1\n",
    "# correlations = poly_df.corrwith(df_NSME1988_dirty_new['adl']).abs()\n",
    "# good_features = correlations[correlations > threshold].index.tolist()\n",
    "\n",
    "# feature_corr = pd.DataFrame({\n",
    "#                                 'feature': good_features,\n",
    "#                                 'correlation': [correlations[f] for f in good_features]\n",
    "#                             }).sort_values('correlation', ascending=False)\n",
    "\n",
    "# print(f\"Features above threshold ({threshold}): {len(good_features)}\")\n",
    "# print(\"Sorted by correlation:\")\n",
    "# print(feature_corr.to_string(index=False))\n",
    "\n",
    "# # Add best features\n",
    "# df_NSME1988_dirty_new = pd.concat([df_NSME1988_dirty_new, poly_df[good_features]], axis=1)\n",
    "\n",
    "# # Display latest stats with new features\n",
    "# DisplayDescriptiveStats(df_NSME1988_dirty_new)\n",
    "\n",
    "# # Could multiply newly created columns by factor and convert to int for performance,\n",
    "# # but that would be overkill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff5a97",
   "metadata": {},
   "source": [
    "#### Heat Maps (plots) of Requested Distributions ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3fadde",
   "metadata": {},
   "source": [
    "#### Normalize all the numeric features - both original and newly created combined features ####\n",
    "    In earlier analysis, we determined that Yeo-Johnson transformation for normalization was the best option\n",
    "    Rather than rerun all the tests again, I'm going to with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fc094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Apply Yeo-Johnson normalization\n",
    "#\n",
    "\n",
    "all_numeric_columns = df_NSME1988_dirty_new.select_dtypes(include=['number']).columns.tolist()\n",
    "print(f'Number of Numeric Columns: {len(all_numeric_columns)}')\n",
    "print(all_numeric_columns)\n",
    "\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "for col in all_numeric_columns:\n",
    "    \n",
    "    # Nan values have been filterd out at this point\n",
    "    # Apply transformations\n",
    "    df_NSME1988_dirty_new[col] = transformer.fit_transform(df_NSME1988_dirty_new[[col]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01fb08",
   "metadata": {},
   "source": [
    "### Feature Scaling - Apply Scaling ###\n",
    "    There are 2 scaling techniques I could user: Standard or Min-Max\n",
    "    After a little research, it appears Standard is the better track since I had/have extreme\n",
    "    Right Skewness and extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33267f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Apply Standard Scaling to dataset\n",
    "#\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_NSME1988_dirty_new[all_numeric_columns] = scaler.fit_transform(df_NSME1988_dirty_new[all_numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d3509",
   "metadata": {},
   "source": [
    "### **Final Thoughts and Recap** ###\n",
    "#### Initial load and EDA analysis uncovering the following initial observations: ####\n",
    "    - The data contained several instances of missing values (mostly centered around the \"number of visits\" feature columns)\n",
    "    - I plotted distributions (via histograms and boxplots) of the original raw data and found serveral of the features\n",
    "        to be heavily right skewed with extreme outliers\n",
    "    - I investigated possible transforms to potentially normalize the data distributions and settled on Yeo-Johnson.\n",
    "    - I noticed some of the columns had nonsensical negative values (visits, income)\n",
    "    - The age column was not in \"actual years\" but was some factorized version (most probably divided by 10)\n",
    "    - The income column was not in \"real dollars\" but was also some factorized version. I couldn't figure out\n",
    "        the exact mapping so I left it alone as it was.\n",
    "\n",
    "#### Data Wrangling ####\n",
    "##### Fix Missing Data #####\n",
    "    I identified the columns with missing data and tested the following imputation techniques to correct the issues:\n",
    "    Median, Mean, KNN and MICE. I tested the results and Median was the clear winner with the lowest Mean Absolute Error (MAE)\n",
    "##### Address Invalid Data #####\n",
    "    There were instances of invalid data with columns having negative values that didn't make sense (income, visits).\n",
    "    To correct the issues I replaced the bad values witn zeros (0).\n",
    "##### Mitigate the effect of extreme outliers #####\n",
    "    I performed analysis to identify that several of the columns indeed had extreme outliers.\n",
    "    I explored the several methods for analysis: IQR, Z-score, Isolation Forest, Local Outlier Factor. \n",
    "    They indeed statistically verified the existence of outliers. This was contributing the heavily right skewed data distributions.\n",
    "    To address the issue I capped (clip()) the column data to the 0.05 and 0.95 percentiles.\n",
    "##### Additional Data Fixes #####\n",
    "    I modified the age so it more resembled a real age by multiplying values by 10. This revealed that this dataset focused\n",
    "    on seniors (age range 65+) rather the pediatric subjects (under the age of 10)\n",
    "##### Binning Exploration #####\n",
    "    I briefly looked the strategy of binning the data into categories (income: low->low-middle=>middle->high-middle=>high)\n",
    "    I ultimately decided against as I weighed the work vs net positive effect. Binning was good for visual analysis of wealth\n",
    "    distribution, but would have complicated Feature Engineering.\n",
    "\n",
    "#### Feature Engineering ####\n",
    "##### Data Encoding #####\n",
    "    I performed One-Hot Encoding (), Ordinal Encoding() and Label Encoding()\n",
    "##### Data Type Conversion #####\n",
    "    Except for Income, float64 types were converted to integers for overall model performance improvements\n",
    "##### Create Combined Feature Columns #####\n",
    "    Examining the dataset, I created some multi-column features which made sense given the context of our data.\n",
    "    (visits per region.., and so on)\n",
    "##### Polynomial Feature Creation #####\n",
    "    As I previously mentioned above, if we were working with a predictive model with a clear target, then\n",
    "    I could have applied PolynomialFeature() creation and filtered the results to the top feature sets.\n",
    "    (Was I confused about the existence/nonexistence of target?)\n",
    "##### Feature Distribution Normalization #####\n",
    "    Based on EDA, I determined that Yeo-Johnson transformation for normalization was the best option\n",
    "##### Feature Scaling #####\n",
    "    I applied Standard Scaling as it is best suited to deal with Right Skewness and extreme outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d5677",
   "metadata": {},
   "source": [
    "## <u>**4. Data Visualization**</u> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9957c8d",
   "metadata": {},
   "source": [
    "## <u>**5. Final Analysis And Report**</u> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71728fec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
